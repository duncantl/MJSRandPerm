<section
	 xmlns:r="http://www.r-project.org">
<title></title>

<para>
Through profiling and examining the code, we know that
<r:func>pairTrials_RandomPerm</r:func> is called many times
and does a lot of work.
We have made several additions to it based on how it is called in the script.
For example, we create a second parameter whose default values is the
data.frame of complete cases. We can then pass this explicitly from the loop calling
<r:func>pairTrials_RandomPerm</r:func> as we need only compute that complete cases data.frame once.
To try to make the function faster, we will try to squeeze out every context-specific information
we have as we will run this many, many times. So every millisecond counts.
</para>
<para>
The original function loops over the unique values of SUBJECTID and then gets
the rows corresponding to that unique value.
This is a group-by operation.  We should use <r:func>tapply</r:func> or <r:func>by</r:func>.
At the end of each iteration of the loop, we append
a data.frame to a cumulating data.frame <r:var>dfMissing_pairedWide</r:var>.
We should try to preallocate this and insert into the relevant rows, or combine
all of these in one operation at the end of the loop, e.g. <r:expr>do.call(rbind, parts)</r:expr>.
</para>

<para>
Ultimately, we will get rid of the loop, the group-by, and the binding of the sub-data.frames.
But for now, we start work on making the loop faster. From that, we will discover the
pattern that can be generalized to all observations.
</para>

<para>
Within each iteration of the loop, we look at the observations for a given
value of SUBJECTID.
We then essentially work separately on the two subsets of these records
based on the value of <r:el>emotion</r:el>.
There are apparently only two possible values A and B.
We create a new variable subclass for all of these
records. This is a random permutation of the indices 1, 2, 3 ..., na for the As
and separately a random permutation of the indices  1, 2, 3, ..., nb for the Bs.
Consider the case where we have 42 observations for the first SUBJECTID, with value 1,
and 32 records with A as the value for emotion, and 10 with value B.
Perhaps by chance, perhaps always, these are ordered so that the 10 B records
are in rows 1, 2, ..., 10 and those for  A in 11 through 42.
The subclass values are
<r:output><![CDATA[
 B  B  B  B  B  B  B  B  B  B  A  A  A  A  A  A  A  A  A  A  A  A  A 
 1  8  6  3 10  4  9  7  2  5 19 12 23 11 25 24 13  4 22 29  9  3  8 
 A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A  A 
32  5  1 14 18 10 17 16 21  6 30 26 15  2 28  7 27 31 20 
]]></r:output>
where we can see the permutation of 1:10 for the Bs and 1:32 for the As.
</para>
<para>
The next step is the call to pivot_wider:
<r:code>
pivot_wider(dfMissing_NoNA_SubjectSubset,
            names_from = emotion,
            names_sep = ".",
            values_from = c(meanAmpNC, ACTOR, presentNumber, presentNumberWeight))
</r:code>
We are going to remove this call to pivot_wider and implement
the same calculations more directly, less generally, and with less overhead
from the quosures, the pipe, etc.
</para>
<para>
This call to pivot_wider returns a tibble with 32 rows and 12 columns.
While the help page for pivot_wider isn't very informative, we can infer
what it does.
The column names of the input data.frame are 
<r:code>
names(dfMissing_NoNA_SubjectSubset)
<r:output><![CDATA[
[1] "SUBJECTID"           "age"                 "emotion"            
[4] "ACTOR"               "presentNumber"       "meanAmpNC"          
[7] "presentNumberWeight" "ageWeight"           "subclass"           
]]></r:output>
</r:code>
pivot_wider will create two new sets of columns for each of the variables identified
in <r:arg>values_from</r:arg> for A and B corresponding to the values of emotion,
i.e. meanAmpNC.A, meanAmpNC.B, ACTOR.A, ACTOR.B.
SUBJECTID and age are the same for all rows.
It will do this by matching the records based on the remaining variables <emdash/>
SUBJECTID, age, ageWeight and subclass.
The first three of these variables have only one unique value. So we are just matching
on subclass, however, doing more work than we need as pivot_wider doesn't know a priori
that these three variables each have a single value.
So what <r:func>pivot_wider</r:func> does is to match
the pair of records for each value of subclass, corresponding to a record for A and
a record for B.
It then puts these values into a single row in the result, with a column
for each of the 4 variables in <r:arg>values_from</r:arg> for the A record and
also 4 columns for the B record.
When a value of subclass, say 11, has no corresponding match, pivot_wider
uses NA for the values. This happens for all of the values of subclass 11:32 in
our example as there are only 10 values of B.  So we get
<r:na/> for all of the values for the B variables for records from 11 through 32.
<r:code>
as.data.frame(dfMissing_pairedWide_SubjectSubset)[, c("subclass", "ACTOR.A", "ACTOR.B")]
<r:output><![CDATA[
   subclass ACTOR.A ACTOR.B
1         1       2       1
2         8       2       2
3         6       4       2
4         3       2       3
5        10       3       4
6         4       1       4
7         9       2       4
8         7       5       5
9         2       5       5
10        5       2       5
11       19       1      NA
12       12       1      NA
]]></r:output>
</r:code>
The columns in the result are
<r:code>
names(dfMissing_pairedWide_SubjectSubset)
<r:output><![CDATA[
 [1] "SUBJECTID"             "age"                  
 [3] "ageWeight"             "subclass"             
 [5] "meanAmpNC.B"           "meanAmpNC.A"          
 [7] "ACTOR.B"               "ACTOR.A"              
 [9] "presentNumber.B"       "presentNumber.A"      
[11] "presentNumberWeight.B" "presentNumberWeight.A"
]]></r:output>
</r:code>
</para>


<para>
The next step is to check the result has two columns
named meanAmpNC.A and meanAmpNC.B. If not, we skip the rest of the computations
in this iteration and essentially ignore this value of SUBJECTID as we don't
append records to the target data frame.
The result would be missing either of these columns if there were
no rows with a value of A for emotion, or a value of B for emotion.
Instead of checking the result of pivot_wider, we can check this when we compute
the number of As and Bs. This would avoid a lot of redundant computation.
</para>

<para>
The next step in the loop is to remove the incomplete cases in the result, in
other words, any of the rows that have a <r:na/> value for any of the .A or .B variables.
Again, we can be more specific here as we need only check two of the columns, e.g.,
ACTOR.A and ACTOR.B:
<r:code>
! (is.na(ACTOR.A) | is.na(ACTOR.B))
</r:code>
However, more importantly, we should not have created these extra rows.
We knew that the rows with subclass values greater than nB (10) would
have no matches. We should have just matched those with a value
of subclass 1:10 directly and ignored the other rows.
</para>
<para>
How can we match these?
We can use  group-by  on subclass,
or subset the A's and B's and use match().
However, we can also order the rows by subclass AND emotion.
<r:code>
df[order(df$subclass, df$emtion),]
</r:code>
We know they have to come in pairs as we created subclass that way.
So we are guaranteed to get pairs for each value of subclass 1, 2, 3, ... nB (10)
with the row with emotion A first and the row for emotion B second in each pair,
and the remaining rows will be those with emotion A and a value of subclass greater than
10 that did not have a match for emotion B.
<r:code>
df[order(df$subclass, df$emotion), c("subclass", "emotion", "ACTOR", "meanAmpNC")]
<r:output><![CDATA[
   subclass emotion ACTOR meanAmpNC
67        1       A     2    13.616
3         1       B     1    -5.762
91        2       A     5    14.832
44        2       B     5    14.088
63        3       A     2     9.268
25        3       B     3     9.740
59        4       A     1     3.951
33        4       B     4    11.180
66        5       A     2    10.511
46        5       B     5    30.878
83        6       A     4    24.315
19        6       B     2    31.076
93        7       A     5    16.165
43        7       B     5    14.840
64        8       A     2    14.575
12        8       B     2     7.988
62        9       A     2     6.126
40        9       B     4    36.138
76       10       A     3    18.168
31       10       B     4    21.219
54       11       A     1    -1.789
52       12       A     1     6.337
58       13       A     1    -7.569
68       14       A     2    27.400
90       15       A     4    21.405
79       16       A     3    10.515
78       17       A     3     9.642
]]></r:output>
</r:code>
</para>
<para>
We can discard rows past 20 = 2 * nB.
The result we want can be obtained by subsetting
rows 1, 3, 5, ..., 19 to get the columns corresponding to emotion A,
and subsetting rows 2, 4, 6, ..., 20 for columns corresponding to B.
We can then combine cbind() these together and fix the names and we have the result
we want. 
</para>

<para>
These computations allow us to avoid the generality and overhead of pivot_wider.
We still have to generate the values of subclass.
We can do this with
<r:code>
subclass = c(sample.int(nB), sample.int(nA))
</r:code>
since the records seem to be ordered.  If they are not,
we can force the ordering in the loop, or better still,
once before the loop that calls <r:func>pairTrials_RandomPerm</r:func>.
</para>
<para>
And of course, some subsets (based on SUBJECTID) will have more A's than B's and others
will have more B's than A's, so to discard the rows
that will not be used we can have
<r:code>
df[ seq_len(2*min(nA, nB)), ]
</r:code>
</para>


<para>
Now that we have seen how to vectorize this for a single value of SUBJECTID,
can we generalize it so that we process all SUBJECTID values in a single
vectorized manner?
We'll order the entire data.frame by SUBJECTID and emotion.
So we will have 42 rows for SUBJECTID == 1 with 32 for A and 10 for B, then 45 rows for SUBJECTID.
We need to compute the permutations for each SUBJECTID and within that the As and the Bs.
This is hard to vectorize, but we can do it with
<r:code>
tt = table(x$SUBJECTID, x$emotion)
x$subclass = unlist(apply(tt, 1, function(x) c(sample.int(x[1]), sample.int(x[2]))))
</r:code>
The call to <r:func>table</r:func> computes the number of As and Bs for each SUBJECTID
and then we loop/aply over the number of SUBJECTID and generate
the permutations for each of A and B for that SUBJECTID and <r:func>unlist</r:func>
the collection of permutations.
The result is a single vector of subclass for the entire data.frame.
We can again reorder the entire data.frame by SUBJECTID, subclass and emotion as we did
above for a single SUBJECTID.
For each SUBJECTID in the ordered data.frame, we have pairs of A and B values for each subclass,
and extra rows for the records for which there were fewer As than Bs or Bs than As.
Next, we discard  these extra rows, and then we will do our cbind() approach on the 1, 3, 5, ...
and 2, 4, 6, ... rows.
To discard the rows, we again need to know about the number of As and Bs for each SUBJECTID.
We'll create a logical vector with as many elements as there are rows in the entire data.frame.
We'll use that to subset the data.frame, dropping the rows we don't want or keeping those
for which we have pairs of A,B, subclass value for each SUBJECTID.
If there are more As than Bs for a given SUBJECTID, we keep 2 times the number of Bs.
Similarly, if there are more Bs than As, we keep 2 times the number of As.
So for each SUBJECTID, we need the minimum of the number of As and Bs and then
create that many <r:true/>s and <r:expr>nA + nB - 2 * min(nA, nB)</r:expr> <r:false/> values
in that order
<r:code>
mask = unlist(apply(tt, 1, function(x){ mn = min(x); rep(c(TRUE, FALSE), c(2*mn, sum(x) - 2*mn))}))
x = x[mask,]
</r:code>
Then we can cbind
<r:code>
i = seq(1, nrow(x)-1L, by = 2)
cbind(x[i, ], x[i + 1L, c("meanAmpNC", "Actor", "presentNumber", "presentNumberWeight")])
</r:code>
</para>


</section>
